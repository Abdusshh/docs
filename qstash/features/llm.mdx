---
title: "LLM"
---

QStash can be used to generate textual responses to one or more
chat messages with different roles, using state of the art open
source large language models.

Large language models are capable of performing wide variety
of text based tasks such as
- answering questions, optionally with history of responses
- generating all kinds of texts such as blogs or documentation
- writing and debugging code
- doing translations between various languages

and many more!

QStash supports the following LLMs from Upstash:

- `meta-llama/Meta-Llama-3-8B-Instruct`
- `mistralai/Mistral-7B-Instruct-v0.2`

It is also possible to use models from other providers like OpenAI,
using the QStash SDKs.

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: upstash(),
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
    ],
});

console.log(completion);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=upstash(),
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
    ],
)

print(completion)
```

```shell curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is a large language model? Explain like I am five."
            }
        ]
    }'
```
</CodeGroup>

It is also possible to simulate a chat session by passing the history
of user and assistant messages to the request.

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: upstash(),
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
        {
            role: "assistant",
            content: "A large language model is a type of artificial intelligence model that is trained on a massive dataset of text to generate human-like language outputs.",
        },
        {
            role: "user",
            content: "What are some use cases that can benefit from LLMs?",
        },
    ],
});

console.log(completion);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=upstash(),
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
        {
            "role": "assistant",
            "content": "A large language model is a type of artificial intelligence model that is trained on a massive dataset of text to generate human-like language outputs.",
        },
        {
            "role": "user",
            "content": "What are some use cases that can benefit from LLMs?",
        },
    ],
)

print(completion)
```

```shell curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is a large language model?"
            },
            {
                "role": "assistant",
                "content": "A large language model is a type of artificial intelligence model that is trained on a massive dataset of text to generate human-like language outputs."
            },
            {
                "role": "user",
                "content": "What are some use cases that can benefit from LLMs?"
            }
        ]
    }'
```
</CodeGroup>

Chat completion responses can also be delivered in chunk streams.

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: upstash(),
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
    ],
    stream: true,
});

for await (const chunk of completion) {
    console.log(chunk);
}
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=upstash(),
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
    ],
    stream=True,
)

for chunk in completion:
    print(chunk)
```

```shell curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -N \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is a large language model?"
            }
        ],
        "stream": true
    }'
```
</CodeGroup>

The code snippets above can also be run with other LLM providers
like OpenAI, just by specifying it as a provider and passing
the API key.


<CodeGroup>
```js JavaScript
import { Client, openai } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: openai({ token: "<OPENAI_API_KEY>" }),
    model: "gpt-3.5-turbo",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
    ],
});

console.log(completion);
```

```python Python
from qstash import QStash
from qstash.chat import openai

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=openai(token="OPENAI_API_KEY"),
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
    ],
)

print(completion)
```
</CodeGroup>

The chat completions endpoint is also compatible with the OpenAI REST API,
so you can use wide variety of tools and libraries, including the official
OpenAI Python client with it.

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://qstash.upstash.io/llm/v1",
    api_key="<QSTASH_TOKEN>",
)

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Test message from the OpenAI client",
        }
    ],
    model="meta-llama/Meta-Llama-3-8B-Instruct",
)

print(completion)
```

## Integrations with QStash APIs

You can publish or enqueue a single or a batch of chat completion
requests, using all of the existing QStash features natively.

For that, the destination `api` must be specified as `llm` with a valid provider,
and the body of the published or enqueued message should contain a valid
chat completion request.

For these integrations, specifying the `Upstash-Callback` header is
required. Also, streaming chat completions cannot be used with them.

All the examples below can also be used with other LLM providers like OpenAI
through QStash SDKs.

### Publishing a Chat Completion Request

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const result = await client.publishJSON({
    api: { name: "llm", provider: upstash() },
    body: {
        model: "meta-llama/Meta-Llama-3-8B-Instruct",
        messages: [
            {
                role: "user",
                content: "Write a hello world program in Rust.",
            },
        ],
    },
    callback: "https://example.com",
});

console.log(result);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

result = q.message.publish_json(
    api={"name": "llm", "provider": upstash()},
    body={
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Write a hello world program in Rust.",
            }
        ],
    },
    callback="https://example.com",
)

print(result)
```

```shell curl
curl "https://qstash.upstash.io/v2/publish/api/llm" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -H "Upstash-Callback: https://example.com" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Write a hello world program in Rust."
            }
        ]
    }'
```
</CodeGroup>

### Enqueueing a Chat Completion Request

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const result = await client.queue({ queueName: "queue-name" }).enqueueJSON({
    api: { name: "llm", provider: upstash() },
    body: {
        model: "meta-llama/Meta-Llama-3-8B-Instruct",
        messages: [
            {
                role: "user",
                content: "Write a hello world program in Rust.",
            },
        ],
    },
    callback: "https://example.com",
});

console.log(result);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

result = q.message.enqueue_json(
    queue="queue-name",
    api={"name": "llm", "provider": upstash()},
    body={
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Write a hello world program in Rust.",
            }
        ],
    },
    callback="https://example.com",
)

print(result)
```

```shell curl
curl "https://qstash.upstash.io/v2/enqueue/queue-name/api/llm" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -H "Upstash-Callback: https://example.com" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is the first prime number?"
            }
        ]
    }'
```
</CodeGroup>

### Sending Chat Completion Requests in Batches

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const result = await client.batchJSON([
    {
        api: { name: "llm", provider: upstash() },
        body: { ... },
        callback: "https://example.com",
    },
    ...
]);

console.log(result);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

result = q.message.batch_json(
    [
        {
            "api": {"name": "llm", "provider": upstash()},
            "body": {...},
            "callback": "https://example.com",
        },
        ...
    ]
)

print(result)
```

```shell curl
curl "https://qstash.upstash.io/v2/batch" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '[
        {
            "destination": "api/llm",
            "body": {...},
            "callback": "https://example.com"
        },
        ...
    ]'
```
</CodeGroup>

### Retrying After Rate Limit Resets

When the rate limits are exceeded, QStash automatically schedules the retry of
publish or enqueue of chat completion tasks depending on the reset time
of the rate limits. That helps with not doing retries prematurely
when it is definitely going to fail due to exceeding rate limits.

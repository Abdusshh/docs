---
title: LlamaParse
---

<Tip>Apply this tutorial to the framework of your choice with [Quickstarts](/vector/sdks/rag-chat/quickstarts/nextjs). </Tip>

[Llama Parse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/) is a document parsing platform that can be used as a content processor in RAGChat.

### Install RAG Chat SDK

Initialize the project and install the required packages:

```bash
npm init es6
npm install dotenv
npm install @upstash/rag-chat
```

### Setup Upstash Redis

Create a Redis database using [Upstash Console](https://console.upstash.com) or [Upstash CLI](https://github.com/upstash/cli) and copy the `UPSTASH_REDIS_REST_URL` and `UPSTASH_REDIS_REST_TOKEN` into your `.env` file.

```shell .env
UPSTASH_REDIS_REST_URL=<YOUR_URL>
UPSTASH_REDIS_REST_TOKEN=<YOUR_TOKEN>
```

### Setup Upstash Vector

Create a Vector index using [Upstash Console](https://console.upstash.com) or [Upstash CLI](https://github.com/upstash/cli) and copy the `UPSTASH_VECTOR_REST_URL` and `UPSTASH_VECTOR_REST_TOKEN` into your `.env` file.

```shell .env
UPSTASH_VECTOR_REST_URL=<YOUR_URL>
UPSTASH_VECTOR_REST_TOKEN=<YOUR_TOKEN>
```

### Setup QStash LLM

Navigate to [QStash Console](https://console.upstash.com/qstash) and copy the `QSTASH_TOKEN` into your `.env` file.

```shell .env
QSTASH_TOKEN=<YOUR_TOKEN>
```

### Setup LlamaParse

Create a LlamaCloud account and get an API key from [Llama Cloud -> API Keys](https://cloud.llamaindex.ai/api-key). Set your LlamaCloud API key as an environment variable:

```bash .env
LLAMA_CLOUD_API_KEY=<YOUR_API_KEY>
```

### Setup the Project

Initialize RAGChat:

```typescript index.ts
import { RAGChat, upstash } from "@upstash/rag-chat";
import "dotenv/config";

const ragChat = new RAGChat({
  model: upstash("meta-llama/Meta-Llama-3-8B-Instruct"),
});
```

Select a PDF file to parse:

```typescript index.ts
const fileSource = "./your-file.pdf";
```

Add context to the RAG Chat with the LlamaParse processor:

```typescript index.ts
await ragChat.context.add({
  options: {
    namespace: "llama-parse-upstash",
  },
  fileSource,
  processor: {
    name: "llama-parse",
    options: { apiKey: process.env.LLAMA_CLOUD_API_KEY },
  },
});
```

Chat with the RAG Chat:

```typescript index.ts
const result = await ragChat.chat("What is the context about?", {
  streaming: false,
  namespace: "llama-parse-upstash",
});
```

### Run

Run the project:

```bash
npx tsx index.ts
```

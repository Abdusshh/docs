---
title: Examples
---

### Update prompt and bypass context

```ts
const result = await ragChat.chat("THIS_IS_USERS_QUESTION", {
  namespace,
  stream: false,
  disableRAG: true,
  promptFn: ({ question, chat_history }) => {
    return `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.
                Chat History:
                ${chatHistory}
                Follow Up Input: ${question}
                Standalone question:`;
  },
});
```


### Check if a message is a question

```ts
const result = await ragChat.chat("Ankara is the capital of Turkey", {
        namespace,
        stream: false,
        disableRAG: true,
        promptFn: ({ question }) => {
          return `Is the following a question? Answer "YES" if it is a question and "NO" if it is not.
              Input: ${question}
              Answer:`;
        },
      });
console.log(result.output) // "NO"
```


### Enable debug logs
```ts
new RAGChat({ debug: true });
```

You will see logs like below:

```sh
{
  "timestamp": 1722950191207,
  "logLevel": "INFO",
  "eventType": "SEND_PROMPT",
  "details": {
    "prompt": "Where is the capital of Japan?"
  }
}
{
  "timestamp": 1722950191480,
  "logLevel": "INFO",
  "eventType": "RETRIEVE_CONTEXT",
  "details": {
    "context": [
      {
        "data": "- Tokyo is the Capital of Japan.",
        "id": "F5BWpryYkkcKLrp-GznwK"
      }
    ]
  },
  "latency": "171ms"
}
{
  "timestamp": 1722950191727,
  "logLevel": "INFO",
  "eventType": "RETRIEVE_HISTORY",
  "details": {
    "history": [
      {
        "content": "Where is the capital of Japan?",
        "role": "user",
        "id": "0"
      }
    ]
  },
  "latency": "145ms"
}
{
  "timestamp": 1722950191828,
  "logLevel": "INFO",
  "eventType": "FORMAT_HISTORY",
  "details": {
    "formattedHistory": "USER MESSAGE: Where is the capital of Japan?"
  }
}
{
  "timestamp": 1722950191931,
  "logLevel": "INFO",
  "eventType": "FINAL_PROMPT",
  "details": {
    "prompt": "You are a friendly AI assistant augmented with an Upstash Vector Store.\n  To help you answer the questions, a context and/or chat history will be provided.\n  Answer the question at the end using only the information available in the context or chat history, either one is ok.\n\n  -------------\n  Chat history:\n  USER MESSAGE: Where is the capital of Japan?\n  -------------\n  Context:\n  - Tokyo is the Capital of Japan.\n  -------------\n\n  Question: Where is the capital of Japan?\n  Helpful answer:"
  }
}
{
  "timestamp": 1722950192593,
  "logLevel": "INFO",
  "eventType": "LLM_RESPONSE",
  "details": {
    "response": "According to the context, Tokyo is the capital of Japan!"
  },
  "latency": "558ms"
}
```

### Add metadata into response

Imagine you are developing a QA RAG Chat and you want to include QA page links as a reference. How would you do it?
You can either integrate them into your LLM calls and let the LLM do the heavy lifting for you. For example:

```ts
await ragChat.chat("Where is the capital of Japan?", {
  onContextFetched(context) {
    return context.map((d) => ({
      ...d,
      data: JSON.stringify({ inputs: d.data, links: d.metadata }),
    }));
  },
});
```

This `onContextFetched` is also useful for modifying your data before feeding it into the LLM.

You can either do this, or simply return metadata alongside your LLM output and handle the metadata yourself.

```ts
await ragChat.context.add({
  type: "text",
  data: "Tokyo is the Capital of Japan.",
  options: { namespace, metadata: { unit: "Samurai" } },
});
await ragChat.context.add({
  type: "text",
  data: "Shakuhachi is a traditional wind instrument",
  options: { namespace, metadata: { unit: "Shakuhachi" } },
});

const result = await ragChat.chat<{ unit: string }>(
  "Where is the capital of Japan?",
  {
    namespace,
  }
);

console.log(result.metadata);
//   {
//       unit: "Samurai",
//     },
//     {
//       unit: "Shakuhachi",
//     },
```
You will also get type safety if you pass a type to `chat`.


### I want to store some metadata in my chat history and access context and chat history

It's really easy to store metadata in your chat history. If you've had a chance to use our RAG Chat in [console](https://console.upstash.com/), you would see that we are storing used models an
and debug info in chat history's metadata. Like this. This is straight from our console:


```ts
  let messages: UpstashMessage[] = []
  let context: PrepareChatResult = []

  const response = await ragChat.chat(question, {
   onChatHistoryFetched(_messages) {
      messages = _messages
      this.metadata = {
        ...this.metadata,
        usedHistory: JSON.stringify(
          messages.map((message) => {
            delete message.metadata?.usedHistory
            delete message.metadata?.usedContext
            return message
          })
        ),
      }
      return _messages
    },
    onContextFetched(_context) {
      context = _context
      this.metadata = { ...this.metadata, usedContext: context.map((x) => x.data.replace("-", "")) }
      return _context
    },
    streaming: true,
    metadata: {
      modelNameWithProvider: `${LLM_MODELS[llmModel].provider}_${llmModel}`,
    },
  })
```

Another trick we use is accessing `history` and `context` by storing it in a different variable on the fly so that way we can show debug info in our UI step by step.

---
title: How to do X
---

### I need to rephrase my chat history and question before actual chat

Sometimes we need to rephrase chat history along with the user's question before making the actual LLM call. Even though it may seem impossible with our current setup, it actually is possible. RAG Chat SDK actually gives us all the building blocks.

```ts
const result = await ragChat.chat("THIS_IS_USERS_QUESTION", {
  namespace,
  stream: false,
  disableRAG: true,
  promptFn: ({ question, chat_history }) => {
    return `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.
                Chat History:
                ${chatHistory}
                Follow Up Input: ${question}
                Standalone question:`;
  },
});
```

Key thing here is to disable RAG so it doesn't try to fetch context from Vector DB. You can repeat that as many times as you want and create yourself a good pipeline to increase accuracy.

### I want to check if a question is actually a question

When building bots, it's important to differentiate given texts and decide whether they are questions or not. Otherwise, the bot tries to answer every chat message.
To overcome that issue, you can do this. Don't forget to disable stream. We currently don't support streaming from `chat` call to `chat` call.

```ts
const result = await ragChat.chat("Ankara is the capital of Turkey", {
        namespace,
        stream: false,
        disableRAG: true,
        promptFn: ({ question }) => {
          return `Is the following a question? Answer "YES" if it is a question and "NO" if it is not.
              Input: ${question}
              Answer:`;
        },
      });
console.log(result.output) // "NO"
```

### I just need chat history for my prompt

Similar to examples above, you can just set your `disableRAG` to false and you're good to go.

### I need to see what's going on under the hood

If you've used Langchain or Llamaindex before, you know there's a lot going on under the hood. And, people require some visibility. We've got you covered.
All you have to do is enable `debug` when initializing your RAG instance.

```ts
new RAGChat({ debug: true });
```

Then, you will be able to see your logs like this:

```sh
{
  "timestamp": 1722950191207,
  "logLevel": "INFO",
  "eventType": "SEND_PROMPT",
  "details": {
    "prompt": "Where is the capital of Japan?"
  }
}
{
  "timestamp": 1722950191480,
  "logLevel": "INFO",
  "eventType": "RETRIEVE_CONTEXT",
  "details": {
    "context": [
      {
        "data": "- Tokyo is the Capital of Japan.",
        "id": "F5BWpryYkkcKLrp-GznwK"
      }
    ]
  },
  "latency": "171ms"
}
{
  "timestamp": 1722950191727,
  "logLevel": "INFO",
  "eventType": "RETRIEVE_HISTORY",
  "details": {
    "history": [
      {
        "content": "Where is the capital of Japan?",
        "role": "user",
        "id": "0"
      }
    ]
  },
  "latency": "145ms"
}
{
  "timestamp": 1722950191828,
  "logLevel": "INFO",
  "eventType": "FORMAT_HISTORY",
  "details": {
    "formattedHistory": "USER MESSAGE: Where is the capital of Japan?"
  }
}
{
  "timestamp": 1722950191931,
  "logLevel": "INFO",
  "eventType": "FINAL_PROMPT",
  "details": {
    "prompt": "You are a friendly AI assistant augmented with an Upstash Vector Store.\n  To help you answer the questions, a context and/or chat history will be provided.\n  Answer the question at the end using only the information available in the context or chat history, either one is ok.\n\n  -------------\n  Chat history:\n  USER MESSAGE: Where is the capital of Japan?\n  -------------\n  Context:\n  - Tokyo is the Capital of Japan.\n  -------------\n\n  Question: Where is the capital of Japan?\n  Helpful answer:"
  }
}
{
  "timestamp": 1722950192593,
  "logLevel": "INFO",
  "eventType": "LLM_RESPONSE",
  "details": {
    "response": "According to the context, Tokyo is the capital of Japan!"
  },
  "latency": "558ms"
}
```

### I'm developing [insert your bot type here] and I need to add some metadata into my results

Let's explain it over a concrete example. Imagine you are developing a QA RAG Chat and you want to include QA page links as a reference. How would you do it?
You can either integrate them into your LLM calls and let the LLM do the heavy lifting for you. For example:

```ts
await ragChat.chat("Where is the capital of Japan?", {
  onContextFetched(context) {
    return context.map((d) => ({
      ...d,
      data: JSON.stringify({ inputs: d.data, links: d.metadata }),
    }));
  },
});
```

This `onContextFetched` is also useful for modifying your data before feeding it into the LLM.

You can either do this, or simply return metadata alongside your LLM output and handle the metadata yourself.

```ts
await ragChat.context.add({
  type: "text",
  data: "Tokyo is the Capital of Japan.",
  options: { namespace, metadata: { unit: "Samurai" } },
});
await ragChat.context.add({
  type: "text",
  data: "Shakuhachi is a traditional wind instrument",
  options: { namespace, metadata: { unit: "Shakuhachi" } },
});

const result = await ragChat.chat<{ unit: string }>(
  "Where is the capital of Japan?",
  {
    namespace,
  }
);

console.log(result.metadata);
//   {
//       unit: "Samurai",
//     },
//     {
//       unit: "Shakuhachi",
//     },
```
You will also get type safety if you pass a type to `chat`.


### I want to store some metadata in my chat history and access context and chat history

It's really easy to store metadata in your chat history. If you've had a chance to use our RAG Chat in [console](https://console.upstash.com/), you would see that we are storing used models an
and debug info in chat history's metadata. Like this. This is straight from our console:


```ts
  let messages: UpstashMessage[] = []
  let context: PrepareChatResult = []

  const response = await ragChat.chat(question, {
   onChatHistoryFetched(_messages) {
      messages = _messages
      this.metadata = {
        ...this.metadata,
        usedHistory: JSON.stringify(
          messages.map((message) => {
            delete message.metadata?.usedHistory
            delete message.metadata?.usedContext
            return message
          })
        ),
      }
      return _messages
    },
    onContextFetched(_context) {
      context = _context
      this.metadata = { ...this.metadata, usedContext: context.map((x) => x.data.replace("-", "")) }
      return _context
    },
    streaming: true,
    metadata: {
      modelNameWithProvider: `${LLM_MODELS[llmModel].provider}_${llmModel}`,
    },
  })
```

Another trick we use is accessing `history` and `context` by storing it in a different variable on the fly so that way we can show debug info in our UI step by step.
